% Chapter 10 - 13

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	SHARED OBJECTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 10 - Shared Objects}
\algo{37}{Shared Object: Centralized Solution}{95}
{
	\begin{items}
		\item Root manages access to shared object.
		\item Time complexity (no theorem): Height of the spanning tree
	\end{items}
}

\algo{38}{Shared Object: Home-Based Solution}{96}
{
	\begin{items}
		\item {\bf Initialization:} {\bf Home Base} known to every node, all accesses routed through Home Base that manages {\bf Locks}.
		\item Suffers from the {\bf Triangular Routing Problem}.
	\end{items}
}

\algo{39}{Shared Object: Arrow Algorithm}{97}
{
	\begin{items}
		\item {\bf Initialization:} Rooted Spanning Tree
		\item Arrows always to the holder of the object. {\bf find by u} Operation to the root, wait for object to arrive, get object.
		\item {\bf Theorem 10.1:} find terminates with message and time complexity $D$.
		\item Works in a complete asynchronous and concurrent setting.
		\item {\bf Lemma 10.2:} An edge ${u, v}$ of the spanning tree is in one of four states:\\
			1.) Pointer from $u$ to $v$ (no message on the edge, no pointer from $v$ to $u$)\\
			2.) Message on the move from $u$ to $v$ (no pointer along the edge)\\
			3.) Pointer from $v$ to $u$ (no message on the edge, no pointer from $u$ to $v$)\\
			4.) Message on the move from $v$ to $u$ (no pointer along the edge)
		\item {\bf Theorem 10.3:} (Arrow, Concurrent Analysis) Let the system be synchronous. 
			Initially, the system is in a quiescent state. At time $0$, a set $S$ of nodes initiates 
			a {\bf find} operation. The message complexity of all {\bf find} operations is $O(log |S|\cdot m^{*})$ 
			where $m^{*}$ is the message complexity of an optimal (with global knowledge) algorithm on the tree.
	\end{items}
}

\algo{40}{Shared Object: Read/Write Caching}{100}
{
	\begin{items}
		\item {\bf Read}-Operation Does not change Arrows, but every node on the way gets copy of object.
		\item {\bf Write}-Operation invalidates chached copies.
		\item Works fine for multiple reads plus at most one write concurrently.
		\item {\bf Theorem 10.4:} Correct, message complexity $3$-competitive (at most a factor $3$ worse than the optimum).
	\end{items}
}

\algo{41}{Shared Object: Pointer Forwarding}{101}
{
	\begin{items}
		\item {\bf Initialization:} Object stored at root $r$, precomputed spanning tree $T$
		\item $u$ requests object, receives it like in the arrow algorithm, but then $u$ is the new root and $r$ a new child of $u$.
	\end{items}
}

\algo{42}{Shared Object: Ivy}{101}
{
	\begin{items}
		\item {\bf Initialization:} Object stored at root $r$, precomputed spanning tree $T$
		\item All nodes on the way to the root $r$ are new children of the requesting node $u$.
		\item Message/Time Complexity: $O(n)$ in {\bf worst case}. Need {\bf Amortized analysis}.
		\item {\bf Theorem 10.5:} If the initial tree is a star, a find request of Algorithm 42 needs 
			at most $log(n)$ steps on average, where $n$ is the number of processors.
		\item {\bf Lemma 10.6:} For $\alpha>1$, $1+log(\alpha-1)/2\le log(\alpha)$.
	\end{items}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	WIRELESS PROTOCOLS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 11 - Wireless Protocols}

\algo{43}{Slotted Aloha}{108}
{
	\begin{items}
		\item {\bf Theorem 11.1:} Alows one node to transmit alone (become a leader) ater expected time $e$.
		\item {\bf Theorem 11.6:} Elects a leader w.h.p. in $O(log(n))$ time slots.
	\end{items}
}

\defi{Text}{Initialization}
{
	Sometimes we want the n nodes to have the IDs ${1, 2, \ldots , n}$. This process is 
	called {\bf initialization}.
}

\theo{11.2}{Non-Uniform Initialization}
{
	If the nodes know $n$, we can initialize them in $O(n)$ time slots.
}

\defi{11.3}{Collision Detection, CD}
{
	Two or more nodes transmitting concurrently is called interference. In a system with collision detection, a 
	receiver can distinguish interference from nobody transmitting. In a system without collision detection, a 
	receiver cannot distinguish the two cases.
}

\algo{44}{RandomizedSplit($b$)}{109}
{
}

\algo{45}{Initialization with Collision Detection}{110}
{
	\begin{items}
		\item Prepares and starts Algorithm 44: RandomizedSplit($b$).
		\item {\bf Theorem 11.4:} Initializes the set of nodes in $O(n)$.
	\end{items}
}

\defi{11.5}{With High Probability}
{
	Some probabilistic event is said to occur {\bf with high probability} ({\bf w.h.p.}), if it happens with 
	a probability $p \ge 1-\frac{1}{n^c}$, where $c$ is a constant. The constant $c$ may be chosen arbitrarily, but it is 
	considered constant with respect to Big-O notation.
}

\algo{46}{Uniform leader election}{111}
{
	\begin{items}
		\item Without CD!
		\item {\bf Theorem 11.7:} elect a leader w.h.p. in $O(log^2(n))$ if $n$ is not known.
	\end{items}
}

\algo{47}{Uniform leader election with CD}{112}
{
	\begin{items}
		\item With CD
		\item {\bf Theorem 11.8:} elect a leader w.h.p. in $O(log(n))$.
	\end{items}
}

\algo{48}{Fast uniform leader election}{113}
{
	\begin{items}
		\item With CD
		\item {\bf Lemma 11.9-11.6:} See in script for details.
		\item {\bf Lemma 11.9:} elects a leader with probability of at least $1-\frac{log(log(n))}{log(n)}$ 
		in time $O(log(log(n))$.
	\end{items}
}

\theo{11.18}{Lower Bound}
{
	Any uniform protocol that elects a leader with probability of at least $1-{\frac{1}{2}}^t$ 
	must run for at least $t$ time slots.
}

\theo{11.19}{Uniform Asynchronous Wakeup without CD}
{
	If nodes wake up in an arbitrary (worst-case) way, any algorithm may take 
	$\Omega(n/log(n))$ time slots until a single node can successfully transmit.
}

{\LARGE Useful Formulas}

See page 116-117 in script for some usefule formulas:
\begin{items}
	\item Union Bound
	\item Markov's Inequality
	\item Chernoff Bound
	\item 2 further unnamed formulas.
\end{items}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	SYNCHRONIZATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 12 - Synchronization}

\defi{12.1}{Valid Clock Pulse}
{
	We call a clock pulse generated at a node $v$ valid if it is generated after $v$ 
	received all the messages of the synchronous algorithm sent to $v$ by its 
	neighbors in the previous pulses.
}

\theo{12.2}{}
{
	If all generated clock pulses are valid according to Defnition 12.1, 
	the above method provides an asynchronous algorithm that behaves exactly 
	the same way as the given synchronous algorithm.
}

\defi{12.3}{Safe Node}
{
	A node $v$ is safe with respect to a certain clock pulse if all messages 
	of the synchronous algorithm sent by $v$ in that pulse have already 
	arrived at their destinations.
}

\lemma{12.4}
{
	If all neighbors of a node $v$ are safe with respect to the current 
	clock pulse of $v$, the next pulse can be generated for $v$.
}

\algo{49}{Synchronizer $\alpha$ (at node $v$)}{120}
{
	\begin{items}
		\item time efficient
		\item No initilaization
		\item {\bf Theorem 12.5:} $T(\alpha)=O(1)$ and $M(\alpha)=O(m)$.
	\end{items}
}

\algo{50}{Synchronizer $\beta$ (at node $v$)}{121}
{
	\begin{items}
		\item message efficient
		\item {\bf Theorem 12.6:} $T(\beta)=O(diameter(T))$ and $M(\beta)=O(n)$\\
			$T_{init}(\beta)=O(n)$ and $M_{init}(\beta)=O(m+n\cdot log(n))$
	\end{items}
}

\algo{51}{Synchronizer $\gamma$ (at node $v$)}{123}
{
	\begin{items}
		\item time and message efficient
		\item {\bf Theorem 12.7:} $T(\gamma)=O(k)$ and $M(\gamma)=O(n+m_C)$\\
			where $k$ is the maximum cluster radius and $m_C$ the number of intercluster edges.
		\item {\bf Corollary 12.11:} Per synchronous round: $T(\gamma)=O(k)$ and $M(\gamma)=O(n^{1+\frac{1}{k}})$
		\item {\bf Corollary 12.11:} $T_{init}(\gamma)=O(n)$ and $M_{init}(\gamma)=O(m+n\cdot log(n))$
	\end{items}
}

\algo{52}{Cluster Construction}{124}
{
	\begin{items}
		\item Growing Ball with ratio $\ge\rho$.
		\item {\bf Theorem 12.8:} Algorithm 52 computes a partition of the network graph into 
		clusters of radius at most $log_{\rho}(n)$. The number of intercluster edges is at 
		most $(\rho -1)\cdot n$.
		\item {\bf Corollary 12.9:} Using $\rho=2$, Algorithm 52 computes a clustering with 
		cluster radius at most $log_2(n)$ and with at most $n$ intercluster edges.
		\item {\bf Corollary 12.10:} Using $\rho=n^{1/k}$, Algorithm 52 computes a clustering with 
		cluster radius at most $k$ and with at most $O(n^{1+\frac{1}{k}})$ intercluster edges.
	\end{items}
}

\theo{12.12}{Global Clock Skew}
{
	The global clock skew (the logical clock difference between any two nodes in the graph) is 
	$\Omega(D)$, where $D$ is the diameter of the graph.
}

\algo{53}{Clock Synchronization $\alpha$ (at node $v$)}{127}
{
	\begin{items}
		\item Update local time when logical time received is larger.
		\item {\bf Lemma 12.13:} Local skew of $\Omega(n)$.
	\end{items}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	PEER-TO-PEER COMPUTING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 13 - Peer-to-Peer Computing}

\defi{}{Architecture Variants}
{
	List of descriptions:
	\begin{items}
		\item Client/Server goes P2P
		\item Unstructured P2P
		\item Hybrid P2P
		\item Structured P2P
	\end{items}
}

\defi{}{Fat Tree}
{
	Every edge connecting a node $v$ to its parent $u$ has a capacity that is equal to all 
	leaves of the subtree routed at $v$.
}

\defi{13.1}{Torus, Mesh}
{
	Let $m$, $d \in \mathbb{N}$. The $(m, d)$-mesh $M(m, d)$ is a graph with node set 
	$V=[m]^d$ and edge set\\
	$$E=\{\{(a_1,\ldots,a_d),(b_1,\ldots,b_d)\}|a_i,b_i \in [m], \sum_{i=1}^d{|a_i-b_i|=1}\}$$
	The $(m, d)$-torus $T(m, d)$ is a graph that consists of an $(m, d)$-mesh and additionally 
	wrap-around edges from nodes $(a_1,\ldots, a_{i-1}, m, a_{i+1},\ldots, a_d)$ to nodes 
	$(a_1,\ldots, a_{i-1}, 1, a_{i+1},\ldots, a_d)$ for all $i \in {1,\ldots, d}$ and all $a_j \in [m]$ with $j \neq i$. 
	In other words, we take the expression $a_i-b_i$ in the sum modulo $m$ prior to computing 
	the absolute value. $M(m, 1)$ is also called a line, $T(m, 1)$ a cycle, and 
	$M(2, d) = T(2, d)$ a $d$-dimensional hypercube. Figure 13.2 presents a linear 
	array, a torus, and a hypercube.
}

\defi{13.2}{Butterfly}
{
	Let $d \in \mathbb{N}$. The $d$-dimensional butterfly $BF(d)$ is a graph with 
	node set $V = [d+1]\times[2]^d$ and an edge set $E = E1 \cup E2$ with 
	$$E1 = \{\{(i, \alpha), (i+1, \alpha)\}|i \in [d], \alpha \in [2]^d\}$$
	and 
	\[
		E2 = \{\{(i, \alpha), (i+1, \beta)\}|i \in [d], \alpha, \beta \in [2]^d, \alpha \textit{ and } \beta \textit{ differ only at the $i$-th position}\}
	\]
	A node set $\{(i, \alpha)|\alpha \in [2]^d\}$ is said to form level $i$ of the butterfly. The 
	$d$-dimensional wrap-around butterfly $W-BF(d)$ is defined by taking the $BF(d)$ and 
	identifying level $d$ with level $0$.
}

\defi{13.3}{Cube-Connected-Cycles}
{
	Let $d \in \mathbb{N}$. The cube-connectedcycles network $CCC(d)$ is a graph with node set 
	$V = \{(a, p)|a \in [2]^d, p \in [d]\}$ and edge set
	\[
		E=\{\{(a,p),(a,(p+1) mod d)\}|a \in [2]^d, p \in [d]\} \cup \{\{(a,p),(b,p)\}|a,b \in [2]^d,p \in [d], a=b \textit{ except for } a_p\}
	\]
}

\defi{13.4}{Shuffle-Exchange}
{
	Let $d \in \mathbb{N}$. The $d$-dimensional shuffle-exchange $SE(d)$ 
	is defined as an undirected graph with node set $V = [2]^d$ and an edge set 
	$E = E_1 \cup E_2$ with 
	\[
		E_1 = \{\{(a_1,\ldots, a_d),(a_1,\ldots,\bar{a}_d)\}|(a_1,\ldots, a_d) \in [2]^d, \bar{a}_d = 1-a_d\}
	\]
	and
	\[
		E_2 = \{\{(a_1,\ldots, a_d),(a_d, a_1,\ldots,a_{d-1})\}|(a_1,\ldots, a_d) \in [2]^d\}
	\]
	Figure 13.5 shows the 3- and 4-dimensional shuffle-exchange graph.
}

\defi{13.5}{DeBruijn}
{
	The $b$-ary DeBruijn graph of dimension $d$ $DB(b, d)$ is an undirected 
	graph $G=(V,E)$ with node set $V=\{v \in [b]^d\}$ and edge set $E$ that 
	contains all edges $\{v,w\}$ with the property that $w \in \{(x, v_1,\ldots, v_{d-1}):x \in [b]\}$, 
	where $v=(v_1,\ldots, v_d)$.
}

\defi{13.6}{Skip List}
{
	The skip list is an ordinary ordered linked list of objects, augmented with 
	additional forward links. The ordinary linked list is the level $0$ of the 
	skip list. In addition, every object is promoted to level $1$ with probability 
	$1/2$. As for level $0$, all level $1$ objects are connected by a linked list. 
	In general, every object on level $i$ is promoted to the next level with 
	probability $1/2$. A special start-object points to the smallest/first object on each level.
	\begin{items}
		\item Search, insert and delete can be implemented in $O(log(n))$ expected time.
	\end{items}
}

\theo{13.7}{}
{
	Every graph of maximum degree $d>2$ and size $n$ must have a diameter of at 
	least $\lceil(log(n))/(log(d-1))\rceil-2$.
}

\algo{Figure 13.7}{Core/Periphery}{145}
{
	
}

\theo{13.8}{DHT with Churn}
{
	We have a fully scalable, efficient P2P system which tolerates $O(log(n))$ 
	worst-case joins and/or crashes per constant time interval. As in other P2P 
	systems, peers have $O(log(n))$ neighbors, and the usual operations 
	(e.g., search, insert) take time $O(log(n))$.
}

\algo{54}{$\mathcal{M}$: forward($\pi$, $v_c$) at peer $v$.}{147}
{
	\begin{items}
		\item The Storage and Multicast Algorithm with the {\bf shared prefix length}.
		\item {\bf Theorem 13.9:} In a static overlay, algorithm $\mathcal{M}$ has the following properties:\\
			(a) It does not induce any duplicate messages (loop-free), and\\
			(b) all peers are reached (complete).
	\end{items}
}

\algo{55}{getNext($v_s$) at peer $v$}{147}
{
	
}