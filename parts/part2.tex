% Chapter 6 - 9

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	LOCALITY LOWER BOUNDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 6 - Locality Lower Bounds}

\algo{27}{Synchronous Algorithm: Canonical Form}{60}
{
\begin{items}
    \item Propagate all initial states in r-neighborhood
    \item Compute output based on all received states
    \item {\bf Lemma 6.1}: Every deterministic, synchronous r-round algorithm
    can be transformed into an algorithm of this form if message size and
    computation is not bounded.
\end{items}
}

\defi{6.2}{r-hop view}
{
    We call the collection of the initial states of all nodes in the
    r-neighborhood of a node v, the r-hop view of v.
}

\coro{6.3}{
    A deterministic $r$-round algorithm $A$ is a function that maps every possible 
    $r$-hop view to the set of possible outputs.
}

\defi{6.4}{Neighborhood Graph}
{
    For a given family of network graphs $\mc{G}$, the r-neighborhood graph
    $\mc{N}_r(\mc{G})$ is defined as follows. The node set of $\mc{N}_r(\mc{G})$
    is the set of all possible labeled r-neighborhoods (i.e., all possible r-hop
    views).
    There is an edge between two labeled r-neighborhoods $\mc{V}_r$ and
    $\mc{V}'_r$ if $\mc{V}_r$ and $\mc{V}'_r$ can be the r-hop views of two
    adjacent nodes.
}

\lemma{6.5}{
    For a given family of network graphs $G$, there is an r-round algorithm that colors 
    graphs of $G$ with $c$ colors iff the chromatic number of the neighborhood graph is 
    $\chi(\mc{N}_r(\mc{G})) \leq c$.
}

\lemma{6.6}{
    Viewed as an undirected graph, the graph $\mc{B}_{2r+1,n}$ is a subgraph
    of the r-neighborhood graph of directed n-node rings with node labels 
    from $[n] = {1, ..., n}$.
}

\defi{6.7}{Deline Graph}
{
    The directed line graph (diline graph) $\mc{DL}(G)$ of a directed graph $G =
    (V, E)$ is defined as follows. The node set of $\mc{DL}(G)$ is
    $V[\mc{DL}(G)] = E$. There is a directed edge $\big((w, x); (y, z)\big)$
    between $(w, x) \in E$ and $(y, z) \in E$ iff $x = y$, i.e., if the first
    edge ends where the second one starts.
}

\lemma{6.8}{
    If $n > k$, the graph $\mc{B}_{k+1,n}$ can be defined recursively as follows: 
    $\mc{B}_{k+1,n} = \mc{DL}(\mc{B}_{k,n})$.
}

\lemma{6.9}{
    For the chromatic numbers $\chi(\mc{G})$ and $\chi(\mc{DL}(\mc{G}))$ of a directed 
    graph $G$ and its diline graph, it holds that
    $\chi\big(\mc{DL}(\mc{G})\big) \geq \log_2 \big(\chi(\mc{G})\big)$.
}

\lemma{6.10}{
    For all $n \geq 1$, $\chi(\mc{B}_{1,n}) = n$. Further, for $n \geq k \geq 2$, 
    $\chi(\mc{B}_{k,n}) \geq \log^{(k-1)} n$.
}

\theo{6.11}{Directed Ring Coloring}{
    Every deterministic, distributed algorithm to color a directed ring with 3 or
    less colors needs at least $(\log^* n)/2 - 1$ rounds.
}

\coro{6.12}{
    Every deterministic, distributed algorithm to compute an MIS of a directed ring 
    needs at least $\log^* n/2 - \O(1)$ rounds.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	ALL-TO-ALL COMMUNICATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 7 - All-to-All Communication}

\defi{7.1}{Minimum Spanning Tree (MST)}{
    Given a weighted graph $G = (V, E, \omega)$. The MST of $G$ is a spanning
    tree $T$ minimizing $\omega(T)$, where $\omega(H) = \sum_{e \in H} \omega_e$
    for any subgraph $H \subseteq G$.
}

\lemma{7.2}{
  For a given graph $G$ let $T$ be an MST, and let $T' \subseteq T$ be a
  subgraph (also known as a fragment) of the MST. Edge $e = (u, v)$ is an
  outgoing edge of $T'$ if $u \in T'$ and $v \notin T'$ (or vice versa). Let the
  minimum weight outgoing edge of the fragment $T'$ be the so-called blue edge
  $b(T')$. Then $T' \cup b(T') \subseteq T$.
}

\algo{28}{Simple MST Construction (at node $v$)}{69}{
    \begin{items}
        \item sequentially grow fragments by adding blue edges to MST
        \item {\bf Theorem 7.3:} On a complete graph, Algorithm 28 computes an
        MST in time $\O (\log n)$.
        \item essentially equivalent to the GHS algorithm 15 in chapter 3.
    \end{items}

}

\algo{29}{Fast MST construction (at node $v$)}{70}{
    \begin{items}
        \item add multiple blue edges per round to the spanning tree
        \item {\bf Theorem 7.5:} Algorithm 29 computes an MST in time $\O (\log
        \log n)$.
    \end{items}
}

\algo{30}{AddEdges(E')}{71}{
    \begin{items}
        \item Given the set of edges $E'$, determine which edges are added to
        the MST in Algorithm 29
        \item Ensures only safe edges are added.
    \end{items}
}

Consult Table 7.1 on page 72 for known time complexity upper and lower bounds
for MST construction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	SOCIAL NETWORKS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 8 - Social Networks}

\defi{8.1}{Cluster Coefficient}{
    The cluster coefficient of a network is defined by the probability that two
    friends of a node are likely to be friends as well, averaged over all the
    nodes.
}

\defi{8.2}{Augmented Grid}{
    We take $n = m^2$ nodes $(i, j) \in V = {1, \ldots ,m}^2$ that are
    identified with the lattice points on an $m \times m$ grid. We define the
    distance between two nodes $(i, j)$ and $(k, \ell)$ as $d \big( (i, j), (k,
    \ell) \big) = | k - i | + |\ell - j|$ as the distance between them on the
    $m \times m$ lattice. The network is modeled using a parameter $\alpha \geq
    0$.
    Each node $u$ has a directed edge to every lattice neighbor. These are the
    local contacts of a node. In addition, each node also has an additional
    random link (the long-range contact). For all $u$ and $v$, the long-range
    contact of $u$ points to node $v$ with probability proportional
    to $d(u, v)^{-\alpha}$ , i.e., with probability 
    $d(u, v)^{-\alpha} / \sum_{w \in V \setminus \{u\}}{d(u, w)^{-\alpha}}$.
    Figure 8.2 (page 78) illustrates the model.
}

\defi{8.3}{With High Probability}{
    Some probabilistic event is said to
    occur with high probability (w.h.p.), if it happens with a probability $p
    \leq 1 - 1/n^c$, where $c$ is a constant. The constant $c$ may be chosen
    arbitrarily, but it is considered constant with respect to Big-O notation.
}

\theo{8.4}{Diameter of Augmented Grid}{
    The diameter of the augmented grid with $\alpha = 0$ is $\O (\log n)$ with
    high probability.
}

\algo{31}{Greedy Routing}{79}{
    \begin{items}
        \item While not at destination go the a neighbor which is closest to the
        destination
        \item {\bf Lemma 8.5:} In the augmented grid, Algorithm 31 finds a
        routing path of length at most $2(m-1) \in \O (\sqrt{n})$
    \end{items}
}

\lemma{8.6}{
  Node $u$'s random link points to a node $v$ with probability
  \begin{items}
    \item $\Theta(1/d(u, v)^\alpha m^{2-\alpha})$ if $\alpha < 2$.
    \item $\Theta(1/(d(u, v)^2 \log n))$ if $\alpha = 2$,
    \item $\Theta(1/d(u,v)^\alpha)$ if $\alpha > 2$.
  \end{items}

  Moreover, if $\alpha > 2$, the probability to see a link of length at least
  $d$ is in $\Theta(1/d^{\alpha - 2})$.
}

\defi{8.7}{Phase}{
    Consider routing from source $s$ to target $t$ and assume
    that we are at some intermediate node $w$. We say that we are in phase $j$
    at node $w$ if the lattice distance $d(w, t)$ to the target node $t$ is
    between $2^j < d(w, t) \leq 2^{j+1}$.
}

\lemma{8.8}{
    Assume that we are in phase $j$ at node $w$ when routing from $s$ to $t$.
    The probability for getting (at least) to phase $j - 1$ in one step
    is at least $\Omega(1 / \log n)$.
}

\theo{8.9}{Expected Lenght of Greedy Routing}{
    Consider the greedy routing path from a node $s$ to a node $t$ on
    an augmented grid with parameter $\alpha = 2$. The expected length of the
    path is $\O(\log^2 n)$.
}

\theo{8.10}{Two Player Rumor Game}{
    In a two player rumor game where both players select one node to initiate
    their rumor in the graph, the first player does not always win.
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	SHARED MEMORY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 9 - Shared Memory}

% probably include all RMW operations

\defi{9.1}{Shared Memory}{
    A shared memory system is a system that consists of asynchronous processes
    that access a common (shared) memory. A process can atomically access a
    register in the shared memory through a set of predefined operations. An
    atomic modification appears to the rest of the system instantaneously. Apart
    from this shared memory, processes can also have some local (private)
    memory.
}

\defi{9.2}{Mutual Exclusion}{
    We are given a number of processes, each
executing the following code sections:


$<Entry> \rightarrow <Critical Section> \rightarrow <Exit> \rightarrow
<Remaining Code>$ \\
A mutual exclusion algorithm consists of code for entry and exit sections, such that the following holds
\begin{items}
    \item {\bf Mutual Exclusion:} At all times at most one process is in the
    critical section.
    \item {\bf No deadlock:} If some process manages to get to the entry
    section, later some (possibly different) process will get to the critical section.
\end{items}
Sometimes we in addition ask for
\begin{items}
    \item {\bf No lockout:} If some process manages to get to the entry section,
    later the same process will get to the critical section.
    \item {\bf Unobstructed exit:} No process can get stuck in the exit section.
\end{items}
}

\algo{32}{Mutual Exclusion: Test-and-Set}{87}{
    \begin{items}
        \item Solution of the mutual exclusion problem using test-and-set. 
        \item {\bf Theorem 9.3} contains the proof of correctness.
    \end{items}
}

\algo{44}{Mutual Exclusion: Peterson's Algorithm}{88}{
    \begin{items}
        \item Solution of the mutual exclusion problem without using any
        Read-Modify-Write Operations but using a spinlock (busy-wait).
        \item {\bf Theorem 9.4} contains the proof of correctness.
        \item Basic version only works for $2$ processes but can be extended to
        $n$ by using a tournament tree.
    \end{items}
}

\defi{9.5}{Collect}{
    There are two operations: A $\textsc{store}(val)$ by process
$p_i$ sets val to be the latest value of its register $R_i$. A \textsc{collect}
operation returns a view, a partial function $V$ from the set of processes to a
set of values, where $V (p_i)$ is the latest value stored by $p_i$, for each
process $p_i$. For a \textsc{collect} operation $cop$, the following validity
properties must hold for every process $p_i$:
\begin{items}
    \item If $V(p_i) = \perp$, then no \textsc{store} operation by $p_i$
        precedes $cop$.
    \item If $V(p_i) = v \neq \perp$, then $v$ is the value of a \textsc{store}
        operation $sop$ of $p_i$ that does not follow $cop$, and there is no
        \textsc{store} operation by $p_i$ that follows $sop$ and precedes $cop$
\end{items}
}

\algo{34}{Collect: Simple (Non-Adaptive) Solution}{89}{
    \begin{items}
        \item directly do \textsc{store} and read every register on
            \textsc{collect}
        \item \textsc{store} has step complexity $1$ and the
            step complexity of a \textsc{collect} operation is $n$.
        \item Not optimal in \textsc{collect} because it always reads all
            register even if they were not written.
    \end{items}

}

\defi{9.6}{Splitter}{
    A splitter is a synchronization primitive with the
    following characteristic. A process entering a splitter exits with either
    \textsc{stop}, \textsc{left}, or \textsc{right}. If $k$ processes enter a
    splitter, at most one process exits with \textsc{stop} and at most $k - 1$
    processes exit with \textsc{left} and \textsc{right}, respectively.
}

\algo{35}{Splitter Code}{90}{
    \begin{items}
        \item Implementation of Definition 9.6 with correctness proof in {\bf
        Lemma 9.7}.
    \end{items}
}

\algo{36}{Adaptive Collect: Binary Tree Algorithm}{92}{
    \begin{items}
        \item Implementation of \textsc{store} and \textsc{collect} using a
        splitter and optimizing the \textsc{collect} operation.
        \item {\bf Lemma 9.8} proofs correctness and shows that the step
        complexity of the first \textsc{store} for a process $p_i$ is $\O(k)$,
        the step complexity of every additional \textsc{store} of $p_i$ is
        $\O(1)$, and the step complexity of \textsc{collect} is $\O(k)$.
        \item The space complexity of this algorithm is exponential in $n$
        because we have to allocate memory for the complete binary tree of depth
        $n-1$.
        \item It is however possible to make the algorithm more space efficient
        by using a randomized splitter leading to a binary tree of depth only
        $\O(\log n)$ with high probability.
    \end{items}
}


\theo{9.9}{Complexity with Splitter Matrix}{
    With the in chapter 9.3.4 introduced algorithm using a {\bf Splitter Matrix}
    instead of a binary tree to optimize space complexity, we get:
    \\
    Let $k$ be the number of participating processes. The step complexity of
    the first store of a process $p_i$ is $\O(k)$, the step complexity of every
    additional store of $p_i$ is $\O(1)$, and the step complexity of collect is
    $\O(k^2)$.
}










