% Chapter 1 - 5

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	VERTEX COLORING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 1 - Vertex Coloring}

\prob{1.1}{Vertex Coloring}
{
	Given an undirected graph $G = (V, E)$, assign a color cu to each vertex $u \in
	V$ such that the following holds: $e = (v,w) \in E \Rightarrow cv \neq cw$.
} 

\defi{1.2}{Node Identifiers}
{
	Each node has a unique identifier, e.g., its IP address. We usually assume that
	each identifier consists of only log n bits if the system has n nodes.
}

\defi{1.3}{Chromatic Number}
{
	Given an undirected Graph $G = (V, E)$, the chromatic number $\chi (G)$ is the
	minimum number of colors to solve Problem 1.1. 
}

\algo{1}{Greedy Sequential}{6}
{
\begin{items}
	\item {\bf Vertex Coloring}
  \item Non-distributed
  \item Centralized
  \item {\bf Theorem 1.5:} is correct and terminates in n ``steps''. The
  algorithm uses at most $\Delta+1$ colors.
\end{items}
}

\defi{1.4}{Degree}
{
	The number of neighbors of a vertex v, denoted by $\delta (v)$, is called the
	degree of v. The maximum degree vertex in a graph G defines the graph degree
	$\Delta (G) = \Delta$.
}

\defi{1.6}{Synchronous Distributed Algorithm}
{
	n a synchronous al- gorithm, nodes operate in synchronous rounds. In each
	round, each processor executes the following steps:
	\begin{enum}
	  \item Do some local computation (of reasonable complexity).
		\item Send messages to neighbors in graph (of reasonable size).
		\item Receive messages (that were sent by neighbors in step 2 of the same
		round).
	\end{enum} 
}

\algo{3}{Reduce}{7}
{
	\begin{items}
		\item {\bf Theorem 1.8:} is correct and has time complexity $n$. The algorithm uses
		at most $\Delta+1$ colors.
	\end{items}
}

\defi{1.7}{Time Complexity}
{
	For synchronous algorithms (as defined in 1.6) the time complexity is the
	number of rounds until the algorithm terminates.
}

\lemma{1.9}
{
	$\chi (Tree) \leq 2$
}

\algo{4}{Slow Tree Coloring}{8}
{
	\begin{items}
		\item Time Complexity: Height of tree (up to n)
		\item Does not need to be synchronous
	\end{items}
}

\defi{1.10}{Asynchronous Distributed Algorithm}
{
	In the asynchronous model, algorithms are event driven (``upon receiving
	message \ldots, do\ldots''). Processors cannot access a global clock. A
	message sent from one processor to another will arrive in finite but unbounded
	time.
}

\defi{1.11}{Time Complexity}
{
	For asynchronous algorithms (as defined in 1.6) the time complexity is the 
	number of time units from the start of the execution to its completion in the
	worst case (every legal input, every execution scenario), assuming that each
	message has a delay of at most one time unit. 
}

\defi{1.12}{Message Complexity}
{
	The message complexity of a syn- chronous or asynchronous algorithm is
	determined by the number of messages exchanged (again every legal input, every
	execution scenario). 
}

\theo{1.13}{Slow Tree Coloring}
{
	Algorithm 4 (Slow Tree Coloring) is correct. If each node knows its parent and
	its children, the (asynchronous) time complexity is the tree height which is
	bounded by the diameter of the tree; the message complexity is $n-1$ in
	a tree with n nodes.
}

\defi{1.14}{Log-Star}
{
	$\forall x \leq 2: log*x:=1 \forall x>2: log*x:=1+log*(log(x))$
}

\algo{5}{``6-Color''}
{
	\begin{items}
		\item {\bf Theorem 1.15:} terminates in $log^{*}(n)$ {\bf time}.
		\item Time Complexity: $O(log^{*}(n))$ 
	\end{items}
}
{10}

\algo{6}{Shift Down}{11}
{
	\begin{items}
		\item {\bf Lemma 1.16:} Preserves coloring legality: also siblings are
		monochromatic
	\end{items}
}

\algo{7}{Six-2-Three}{11}
{
	\begin{items}
		\item {\bf Theorem 1.17:} colors a tree with three colors in $O(log^{*}(n))$.
	\end{items}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	LEADER ELECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 2 - Leader Election}

\desc{
\begin{items}
	\item Ring topology
\end{items}
}

\prob{2.1}{Leader Election}
{
	Each node eventually decides whether it is a leader or not, subject to the
	constraint that there is exactly one leader.
}

\defi{2.2}{Anonymous}
{
	A system is anonymous if nodes do not have unique identifiers.
}

\defi{2.3}{Uniform}
{
	An algorithm is called uniform if the number of nodes n is not known to the
	algorithm (to the nodes, if you wish). If n is known, the algorithm is called
	non-uniform. 
}

\lemma{2.4}
{
	After round $k$ of any deterministic algorithm on an anonymous ring, each node
	is in the same state $s_k$.
}

\theo{2.5}{Anonymous Leader Election}
{
	Deterministic leader election in an anonymous ring is impossible.\\
	{\it (Also holds for other symmetric network topologies (e.g., com- plete
	graphs, complete bipartite graphs), but does not hold for randomized
	algorithms.)} 
}

\algo{8}{Clockwise}{17}
{
	\begin{items}
		\item {\bf Theorem 2.6:} is correct. The {\bf time complexity} is $O(n)$. The
		{\bf message complexity} is $O(n2)$.
	\end{items}
}

\algo{9}{Radius Growth}{18}
{
	\begin{items}
		\item {\bf Theorem 2.7:} is correct. The {\bf time complexity} is $O(n)$.
		The {\bf message complexity} is $O(n log n)$.
		\item Asynchronous
		\item Uniform
	\end{items}
}

\defi{2.8}{Execution}
{
	Definition 2.8 (Execution). An execution of a distributed algorithm is a list
	of events, sorted by time. An event is a record (time, node, type, message),
	where type is ``send'' or ``receive''.
}

\defi{2.9}{Open Schedule}
{
	A schedule is an execution chosen by the scheduler. An open (undirected) edge
	is an edge where no message traversing the edge has been received so far. A
	schedule for a ring is open if there is an open edge in the ring.
}

\lemma{2.10}
{
	Given a ring $R$ with two nodes, we can construct an open schedule in which at
	least one message is received. The nodes cannot distinguish this schedule from
	one on a larger ring with all other nodes being where the open edge is. 
}

\lemma{2.11}
{
	By gluing together two rings of size $n/2$ for which we have open schedules, we
	can construct an open schedule on a ring of size n. If $M(n/2)$ denotes the
	number of messages already received in each of these schedules, at least
	$2M(n/2) + n/4$ messages have to be exchanged in order to solve leader
	election.
}

\lemma{2.12}
{
	Any uniform leader election algorithm for asynchronous rings has at least
	message complexity $M(n) \geq n (log(n) + 1)$.
}

\theo{2.13}{Asynchronous Leader Election Lower Bound}
{
	Any uniform leader election algorithm for asynchronous rings has
	$\Omega (n*log(n))$ message complexity.
}

\algo{10}{Synchronous Leader Election}{21}
{
	\begin{items}
		\item Non-uniform (i.e. the ring size n is known)
		\item Every node starts at same time
		\item Mimimum identifies (integers) becomes the leader
		\item {\bf Time Complexity:} $m*n$, where $m$ is the minimum identifier
		\item {\bf Message Complexity:} n. 
	\end{items}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	TREE ALGORITHMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 3 - Tree Algorithms}

\defi{3.1}{Broadcast}
{
	A broadcast operation is initiated by a single processor, the source. The
	source wants to send a message to all other nodes in the system. 
}

\defi{3.2}{Distance, Radius, Diameter}
{
	The {\bf distance} between two nodes $u$ and $v$ in an undirected graph $G$ is
	the number of hops of a minimum path between $u$ and $v$. The {\bf radius of a
	node} $u$ is the maximum distance between $u$ and any other node in the graph.
	The {\bf radius of a graph} is the minimum radius of any node in the graph. The
	{\bf diameter} of a graph is the maximum distance between two arbitrary
	nodes.\\
	Relation between radius and diameter: $R \leq D \leq 2R$ 
}

\theo{3.3}{Broadcast Lower Bound}
{
	The message complexity of broadcast is at least $n-1$. The source's radius is
	a lower bound for the time complexity.
}

\defi{3.4}{Clean}
{
	A graph (network) is clean if the nodes do not know the topology of the graph.
}

\theo{3.5}{Clean Broadcast Lower Bound}
{
	For a clean network, the num- ber of edges is a lower bound for the broadcast
	message complexity.
}

\algo{11}{Flooding}{24}
{
	\begin{items}
		\item
	\end{items}
}

\algo{12}{Echo}{25}
{
	\begin{items}
		\item {\bf Message Complexity: } $n-1$
		\item Together with flooding the message complexity is $O(m)$, where $m=|E|$
		is the number of edges in the graph.
		\item {\bf Time Complexity: } Depth of the spanning tree.
	\end{items}
}

\algo{13}{Dijkstra BFS}{26}
{
	\begin{items}
		\item {\bf Spanning Tree}
		\item Always add the closest node.
		\item {\bf Theorem 3.6:} The {\bf time complexity} is $O(D2)$, the
		{\bf message complexity} is $O(m + nD)$, where $D$ is the diameter of the
		graph, $n$ the number of nodes, and $m$ the number of edges.
	\end{items}
}

\algo{14}{Bellman-Ford BFS}{26}
{
	\begin{items}
		\item {\bf Spanning Tree}
		\item Simply keep the distance to the root accurate.
		\item Border Gateway Protocol
		\item {\bf Theorem 3.7:} The {\bf time complexity} is $O(D)$, the
		{\bf message complexity} is $O(nm)$, where $D$ is the diameter of the
		graph, $n$ the number of nodes, and $m$ the number of edges.
	\end{items}
}

\defi{3.8}{MST (Minimum spanning tree)}
{
	Given a weighted graph $G = (V, E, \omega)$, the MST of $G$ is a spanning tree
	T minimizing $\omega (T)$, where $\omega (G') = \sum_{e \in G'}\omega_e$ for
	any subgraph $G' \subseteq G$.
}

\defi{3.9}{Blue Edge}
{
	Let T be a spanning tree of the weighted graph $G$ and $T' \subseteq T$ a
	subgraph of $T$ (also called a fragment). Edge $e = (u,v)$ is an outgoing edge
	of $T'$ if $u \in T'$ and $v \not\in T'$ (or vice versa). The minimum weight
	outgoing edge $b(T')$ is the so-called blue edge of $T'$. 
}

\lemma{3.10}
{
	For a given weighted graph G (such that no two weights are the same), let $T$
	denote the MST, and $T'$ be a fragment of $T$. Then the blue edge of $T'$ is
	also part of $T$, i.e., $T' \cup b(T') \subseteq T$.
}

\algo{15}{GHS (Gallager-Humblet-Spira)}{28}
{
	\begin{items}
		\item {\bf Theorem 3.11:} The {\bf time complexity} is $O(n log
		n)$, the {\bf message complexity} is $O(m log n)$.
		\item Directly solves leader election in general graphs: The leader is simply
		the last surviving root.
	\end{items}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	DISTRIBUTED SORTING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 4 - Distributed Sorting}

\defi{4.1}{Sorting}
{
	We choose a graph with n nodes v1, \ldots, vn. Initially each node stores a
	value. After applying a sorting algorithm, node vk stores the kth smallest
	value.\\
	Star Topology: Sorting takes $O(1)$.
}

\defi{4.2}{Node Contention}
{
	n each step of a synchronous algorithm, each node can only send and receive
	$O(1)$ messages containing $O(1)$ values, no matter how many neighbors the node
	has.\\
	Star Topology: Sorting takes $O(n)$.
}

\algo{16}{Odd/Even Sort}{32}
{
	\begin{items}
		\item {\bf Theorem 4.4:} sorts correctly in n steps.
	\end{items}
}

\lemma{4.3}
{
	{\bf 0-1 Sorting Lemma:} If an oblivious comparison-exchange algorithm sorts
	all inputs of $0's$ and $1's$, then it sorts arbitrary inputs.
}

\algo{17}{Shearsort}{33}
{
	\begin{items}
		\item {\bf Theorem 4.5:} sorts n values in $\sqrt(n)(log n + 1)$ time in
		snake-like order.
	\end{items}
}

\defi{4.6}{Sorting Network}
{
	A comparator is a device with two inputs $x$, $y$ and two outputs $x'$, $y'$
	such that $x' = min(x,y)$ and $y' = max(x,y)$. We construct so-called
	comparison networks that consist of wires that connect  comparators (the
	output port of a comparator is sent to an input port of another comparator).
	Some wires are not connected to comparator outputs, and some are not connected
	to comparator inputs. The first are called input wires of the comparison
	network, the second output wires. Given n values on the input wires, a sorting
	network ensures that the values are sorted on the output wires. We will also
	use the term width to indicate the number of wires in the sorting network. 
}

\defi{4.7}{Depth}
{
	he depth of an input wire is $0$. The depth of a comparator is the maximum
	depth of its input wires plus one. The depth of an output wire of a comparator
	is the depth of the comparator. The depth of a comparison network is the
	maximum depth (of an output wire). 
}

\defi{4.8}{Bitonic Sequence}
{
	A bitonic sequence is a sequence of numbers that first monotonically
	increases, and then monotonically decreases, or vice versa.
}

\algo{18}{Half Cleaner}{35}
{
	\begin{items}
		\item {\bf Lemma 4.9:} Feeding a bitonic sequence into a half cleaner, the
		half cleaner cleans (makes all-0 or all-1) either the upper or the lower half
		of the n wires. The other half is bitonic.
	\end{items}
}

\algo{19}{Bitonic Sequnence Sorter}{35}
{
	\begin{items}
		\item {\bf Lemma 4.10:} A bitonic sequence sorter of width $n$
		sorts bitonic sequences. It has depth $log n$.
	\end{items}
}

\algo{20}{Merging Network}{36}
{
	\begin{items}
		\item {\bf Lemma 4.11:} A merging network of width $n$ merges two
		sorted input sequences of length $n/2$ each into one sorted sequence of
		length $n$.
	\end{items}
}

\algo{21}{Batcher's ``Bitonic'' Sorting Network}{36}
{
	\begin{items}
		\item {\bf Theorem 4.12:} A sorting network sorts an arbitrary sequence of
		$n$ values. It has depth $O(log^{2} n)$.
		\item Can be simulated by a Butterfly network and other hypercubic networks.
	\end{items}	
}

\defi{4.13}{Distributed Counting}
{
	A distributed counter is a variable that is common to all processors in a
	system and that supports an atomic test-and-increment operation. The
	operation delivers the system’s counter value to the requesting processor and
	increments it.
}

\defi{4.14}{Balancer}
{
	A balancer is an asynchronous flip-flop which forwards messages that arrive on
	the left side to the wires on the right, the first to the upper, the second to
	the lower, the third to the upper, and so on.
}

\algo{22}{Bitonic Counting Network}{38}
{
	\begin{items}
		\item {\bf Theorem 4.20:} In a quiescent state, the $w$ output wires of a
		bitonic counting network of width $w$ have the step property.
		\item {\bf Lemma 4.23:} The bitonic counting network is not linearizable.
	\end{items}
}

\defi{4.15}{Step Property}
{
	A sequence $y_0$,$y_1$,\ldots,$y_{w-1}$ is said to have the step property,if $0
	\leq y_i-y_j \leq 1$, for any $i<j$.
}

\fact{4.16}
{
	For a balancer, we denote the number of consumed messages on the $i^{th}$ input
	wire with $x_i$, $i = 0$, $1$. Similarly, we denote the number of sent messages
	on the ith output wire with $y_i$ , $i = 0$, $1$. A balancer has these
	properties:
	\begin{enum}
		\item A balancer does not generate output-messages; that is, $x_0 + x_1 \geq
		y_0 + y_1$ in any state.
		\item Every incoming message is eventually forwarded. In other words, if we
		are in a quiescent state (no message in transit), then $x_0 + x_1 = y_0 +
		y_1$.
		\item The number of messages sent to the upper output wire is at most one
		higher than the number of messages sent to the lower output wire: in any
		state $y_0 = \lceil (y_0 + y_1)/2\rceil$ (thus $y_1 = \lfloor (y_0 +
		y_1)/2\rfloor$).
	\end{enum}
}

\fact{4.17}
{
	If a sequence $y_0$, $y_1$, \ldots , $y_{w-1}$ has the step property, 
	\begin{enum}
		\item then all its subsequences have the step property.
		\item then its even and odd subsequences satisfy\\
		$\sum_{i=0}^{w/2-1}y_{2i}= \lceil \frac{1}{2} \sum_{i=0}^{w-1} y_i \rceil$ and 
		$\sum_{i=0}^{w/2-1}y_{2i+1}= \lfloor \frac{1}{2} \sum_{i=0}^{w-1} y_i \rfloor$
	\end{enum}
}

\fact{4.18}
{
	If two sequences $x_0$, $x_1$,\ldots,$x_{w-1}$ and $y_0$,
	$y_1$, \ldots, $y_{w-1}$ have the step property,
	\begin{enum}
		\item and $\sum_{i=0}^{w-1} x_i = \sum_{i=0}^{w-1} y_i$, then $x_i = y_i$
		for $i=0$,\ldots,$w-1$.
		\item and $\sum_{i=0}^{w-1} x_i = \sum_{i=0}^{w-1} y_i+1$ then there exists
		a unique $j$ ($j = 0$,$1$,\ldots,$w-1$) such that $x_j = y_j+1$, and $x_i
		=y_i$ for $i=0$,\ldots,$w-1$, $i \neq j$.
	\end{enum}
}

\lemma{4.19}
{
	Let M[w] be a Merger of width w. In a quiescent state (no mes- sage in
	transit), if the inputs $x_0$, $x_1$, \ldots , $x_{w/2-1}$ resp. $x_{w/2}$,
	$x_{w/2+1}$, \ldots, $x_{w-1}$ have the step property, then the output $y_0$,
	$y_1$, \ldots, $y_{w-1}$ has the step property. 
}

\theo{4.21}{Counting vs. Sorting}
{
	If a network is a counting network then it is also a sorting network, but not
	vice versa.
}

\defi{4.22}{Linearizable}
{
	A system is linearizable if the order of the values assigned reflects the
	real-time order in which they were requested. More formally, if there is a
	pair of operations $o_1$, $o_2$, where operation $o_1$ terminates be- fore
	operation $o_2$ starts, and the logical order is ``$o_2$ before $o_1$'', then a
	distributed system is not linearizable.
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	MAXIMAL INDEPENDENT SET
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture{Chapter 5 - Maximal Independent Set}

\desc{
	\begin{items}
		\item (First) randomized algorithm.
	\end{items}
}

\defi{5.1}{Independent Set}
{
	Given an undirected Graph $G = (V, E)$ an independent set is a subset of nodes
	$U \subseteq V$ , such that no two nodes in $U$ are adjacent. An
	{\bf independent set is maximal} if no node can be added without violating
	independence. An independent set of maximum cardinality is called maximum.
}

\algo{23}{Slow MIS}{46}
{
	\begin{items}
		\item {\bf Theorem 5.2:} features a {\bf time complexity} of $O(n)$ and
		a {\bf message complexity} of $O(m)$.
	\end{items}
}

\coro{5.3}
{
	Given a coloring algorithm that needs C colors and runs in time T, we can
	construct a MIS in time C + T.\\
	{\it (We first choose all nodes of the first color. Then, for each additional
	color we add ``in parallel'' (without conflict) as many nodes as possible.)}
}

\algo{24}{Fast MIS}{47}
{
	\begin{items}
		\item {\bf Lemma 5.4 - Joining MIS:} A node v joins the MIS in Step 2 of the
		Fast MIS with probability $p \geq 1$.
		\item {\bf Lemma 5.5 - Good Nodes:} A node v is called good if\\
		$\sum_{w\in N(v)} \frac{1}{2d(w) \geq \frac{1}{6}}$\\
		Otherwise we call $v$ a bad node. A good node will be removed in Step 3 with
		probability $p \geq \frac{1}{36}$.
		\item {\bf Lemma 5.6 - Good Edges:} An edge $e = (u, v)$ is called bad if both
		$u$ and $v$ are bad; else the edge is called good. The following holds: At any
		time at least half of the edges are good.
		\item {\bf Theorem 5.8:} terminates in expected {\bf time} $O(log n)$ (, even
		with high probabilty).
	\end{items}	
}

\lemma{5.7}
{
	A bad node has outdegree (number of edges pointing away from bad node) at least
	twice its indegree (number of edges pointing towards bad node).
}

\algo{25}{Fast MIS 2}{50}
{
	\begin{items}
		\item {\bf Lemma 5.10:} In a single phase, we remove at least half of the
		edges in expectation.
		\item {\bf Theorem 5.11:} terminates after {\bf time} at most $3 log_{4/3} m +
		1 \in O(log n)$ phases in expectation.\\
		{\bf Corollary 5.14:} terminates w.h.p. in O(log n) time.
	\end{items}
}

\theo{5.9}{Linearity of Expectation}
{
	Let $X_i$, $i = 1$, \ldots , $k$ denote random variables, then\\
	$E[\sum_{i} X_i] = \sum_{i} E[X_i]$.
}

\defi{5.12}{With High Probability (W.h.p.)}
{
	We say that an algorithm terminates w.h.p. (with high probability) within
	$O(t)$ time if it does so with probability at least $1 - 1/n^c$ for any choice
	of $c \geq 1$.
	Here c may affect the constants in the Big-O notation because it is considered
	a ``tunable constant'' and usually kept small.
}

\defi{5.13}{Chernoff's Bound}
{
	Let $X = \sum_{i=1}^{k} X_i$ be the sum of $k$ independent 0-1 random
	variables. Then Chernoff's bound states that w.h.p.\\
	$|X - E[X]| \in O(log n + \sqrt{E[X] log n})$ 
}

\defi{5.15}{Matching}
{
	Given a graph $G = (V, E)$ a matching is a subset of edges $M \subseteq E$,
	such that no two edges in $M$ are adjacent (i.e., where no node is adjacent to
	two edges in the matching). A matching is maximal if no edge can be added
	without violating the above constraint. A matching of maximum cardinality is
	called maximum. A matching is called perfect if each node is adjacent to an
	edge in the matching. 
}

\defi{5.16}{Approximation}
{
	An approximation algorithm A for a maximization problem $\Pi$ has an
	approximation factor of $r$ if the following condition holds for all instances $I \in \Pi$:\\
	$\frac{OPT(I)}{A(I)} \leq r$.
}

\algo{26}{General Graph Coloring}{54}
{
	\begin{items}
		\item {\bf Theorem 5.17:} $(\Delta + 1)$-colors an arbitrary graph in $O(log
		n)$ time, with high probability, $\Delta$ being the largest degree in the
		graph.
	\end{items}
}

\defi{5.18}{Bounded Independence}
{
	$G = (V, E)$ is of bounded independence, if each neighborhood contains at
	most a constant number of independent (i.e., mutually non-adjacent) nodes. 
}

\defi{5.19}{(Minimum) Dominating Set}
{
	A dominating set is a subset of the nodes such that each node is in the set or
	adjacent to a node in the set. A minimum dominating set is a dominating set
	containing the least possible number of nodes.
}

\coro{5.20}
{
	On graphs of bounded independence, a constant-factor approximation to a
	minimum dominating set can be found in time $O(log n)$ w.h.p.
}









